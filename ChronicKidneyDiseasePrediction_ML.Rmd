---
title: "ChronicKidneyDiseasePrediction_ML"
author: "Shruti"
date: "February 25, 2016"
output: html_document
---

Machine Learning Classification Problem example. Uses different machine learning algorithms to classify patients with Chronic Kidney Disease or not based on 24 features. 
https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease

**Load Required Packages**
```{r}
library(RWeka)
library(caret)
# libraries for partition trees
library(rpart)
library(rpart.plot)
library(rattle)
```

**load data** 
```{r}
## load data
setwd("/Users/shruti/Desktop/WorkMiscellaneous/MachineLearning/UCI_ML/Chronic_Kidney_Disease")
Chronic_Kidney_Disease <- read.arff("chronic_kidney_disease.arff")

# data munging
dim(Chronic_Kidney_Disease)
head(Chronic_Kidney_Disease)
colnames(Chronic_Kidney_Disease)
summary(Chronic_Kidney_Disease)

# % data missing in each column
apply(Chronic_Kidney_Disease,2,function(i) {(sum(is.na(i))/nrow(Chronic_Kidney_Disease))*100})
# samples with no missing data
sum(complete.cases(Chronic_Kidney_Disease))
# samples with no missing data after removing columns which have more than 25% of data missing
sum(complete.cases(Chronic_Kidney_Disease[,-c(6,17,18)]))

### remove rows with any missing value
Chronic_Kidney_Disease2 <- Chronic_Kidney_Disease[complete.cases(Chronic_Kidney_Disease),]
dim(Chronic_Kidney_Disease2)
sapply(Chronic_Kidney_Disease2[1,],class)
apply(Chronic_Kidney_Disease[,c(3:9,19:25)],2,table)
```

**split data**
```{r}
set.seed(1)
training_index <- createDataPartition(Chronic_Kidney_Disease2$class,p=0.8,list=F)
# training_set <- Chronic_Kidney_Disease2[training_index,]
# test_set <- Chronic_Kidney_Disease2[-training_index,]
training_set <- droplevels(Chronic_Kidney_Disease2[training_index,])
test_set <- droplevels(Chronic_Kidney_Disease2[-training_index,])
dim(training_set); dim(test_set)
```

**exploratory data analysis**
```{r}
# remove zero covaritates (features with no variability)
nearZeroVar(training_set,saveMetrics = T)
near_zero_covariates <- nearZeroVar(training_set)
head(near_zero_covariates)
if(length(near_zero_covariates)>0)
{
  training_set2 <- training_set[,-near_zero_covariates]
  test_set2 <- test_set[,-near_zero_covariates]
} else {
  training_set2 <- training_set
  test_set2 <- test_set
}
dim(training_set2); dim(test_set2)

# correlated features
# all features should be numeric to calculate correlation
sapply(training_set2[1,],class)
numeric_set <- which(sapply(training_set2[1,],class)=="numeric")

feature_correlation <- cor(training_set2[,numeric_set])
# search through a correlation matrix and returns a vector of integers corresponding to columns to remove to reduce pair-wise correlations.
findCorrelation(feature_correlation,0.75,verbose=T,names=T)
high_correlation <- findCorrelation(feature_correlation,0.75,verbose=T,names=T)
high_correlation
grep(c(high_correlation[1]),colnames(training_set2))
grep(c(high_correlation[2]),colnames(training_set2))
if(length(high_correlation)>0)
{
  training_set3 <- training_set2[,-c(11,15)]
  test_set3 <- test_set2[,-c(11,15)]
}else{
  training_set3 <- training_set2
  test_set3 <- test_set2
}
dim(training_set3); dim(test_set3)

# plots
# PCA
numeric_set2 <- which(sapply(training_set3[1,],class)=="numeric")
pc <- prcomp(training_set3[,numeric_set2],center=T,scale=T)
plot(pc,type="l")
pc$rotation[order(-abs(pc$rotation[,"PC1"])),]

for(i in seq_along(colnames(training_set3[,1:21])))
{
  plot(training_set3$class,training_set3[,i],main=colnames(training_set3)[i])
}

#featurePlot(training_set3[,1:21],training_set3$class, plot="pairs")
```

**model building**
```{r}
# k-fold cross validation
train_control <- trainControl(method="cv", number=5, savePredictions = T)

# model building
# svm - linear
set.seed(1)
svm_lm_model <- train(class ~ .,data = training_set3, trControl=train_control, method = "svmLinear",preProcess = c("center", "scale","pca"))

svm_lm_model
varImp(svm_lm_model)
plot(varImp(svm_lm_model))

# svm - rbf kernel
set.seed(1)
svm_rbf_model <- train(class ~ .,data = training_set3, trControl=train_control, method = "svmRadial",preProcess = c("center", "scale","pca"))

# logistic regression
set.seed(1)
glm_model <- train(class ~ .,data = training_set3, trControl=train_control, method = "glm",family=binomial,maxit=100)
# possible warnings with glm:
#1. "glm.fit: algorithm did not converge"; use maxit much greater than default value of 25"
#2. "glm.fit: fitted probabilities numerically 0 or 1 occurred" indicates that you should use penalized regression. Your fit is exactly collinear.
#3. "prediction from a rank-deficient fit may be misleading in:" indicates If the fit is rank-deficient, some of the columns of the design matrix will have been dropped. Prediction from such a fit only makes sense if 'newdata' is contained in the same subspace as the original data.

# classification Trees
set.seed(1)
rpart_model <- train(class ~ .,data = training_set3, trControl=train_control, method = "rpart")
plot(varImp(rpart_model))
# plot classification trees
fancyRpartPlot(rpart_model$finalModel)
summary(subset(training_set3,class=="ckd",hemo))
summary(subset(training_set3,class=="notckd",hemo))
plot(training_set3$class,training_set3$hemo)
plot(varImp(rpart_model))
plot(rpart_model)

# random forest
set.seed(1)
rf_model <- train(class ~ .,data = training_set3, trControl=train_control, method = "rf",prox=T)
plot(rf_model)

# boosting with tres
set.seed(1)
gbm_model <- train(class ~ .,data = training_set3, trControl=train_control, method = "gbm", verbose=F)
plot(gbm_model)

linear discriminant analysis
set.seed(1)
lda_model <- train(class~., data=training_set3, trControl=train_control, method="lda")

# TO DO: following gives warnings:
# naive bayes
# fix the parameters of the algorithm
grid <- expand.grid(.fL=c(0), .usekernel=c(TRUE))
set.seed(1)
nb_model <- train(class~., data=training_set3, trControl=train_control, method="nb", tuneGrid=grid)

# collect resamples
train_results <- resamples(list(SVM_LM=svm_lm_model,SVM_RBF=svm_rbf_model,RPART=rpart_model,GBM=gbm_model,RF=rf_model,NB=nb_model,LDA=lda_model,GLM=glm_model))
# summarize the distributions
summary(train_results)
# boxplots of results
bwplot(train_results)
# the above results suggest that random forest model performs best on the training data.
lapply((list(SVM_LM=svm_lm_model,SVM_RBF=svm_rbf_model,RPART=rpart_model,GBM=gbm_model,RF=rf_model,NB=nb_model,LDA=lda_model,GLM=glm_model)),varImp)
```

**EVALUATE MODEL ACCURACY ON TEST SET**
```{r}
#Ideally, you select model that performs best on training data and evaluate on test set. I am doing for all models just for illustration 
test_pred_svm_lm <- predict(svm_lm_model, newdata=test_set3)
confusionMatrix(data=test_pred_svm_lm, test_set3$class)

test_pred_svm_rbf <- predict(svm_rbf_model, newdata=test_set3)
confusionMatrix(data=test_pred_svm_rbf, test_set3$class)

test_pred_rpart <- predict(rpart_model, newdata=test_set3)
confusionMatrix(data=test_pred_rpart, test_set3$class)

test_pred_gbm <- predict(gbm_model, newdata=test_set3)
confusionMatrix(data=test_pred_gbm, test_set3$class)

test_pred_rf <- predict(rf_model, newdata=test_set3)
confusionMatrix(data=test_pred_rf, test_set3$class)

test_pred_nb <- predict(nb_model, newdata=test_set3)
confusionMatrix(data=test_pred_nb, test_set3$class)

test_pred_lda <- predict(lda_model, newdata=test_set3)
confusionMatrix(data=test_pred_lda, test_set3$class)
```

