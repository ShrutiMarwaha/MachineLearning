---
title: "ChronicKidneyDiseasePrediction_ML"
author: "Shruti"
date: "February 25, 2016"
output: html_document
---

Machine Learning Classification Problem example. Uses different machine learning algorithms to classify patients with Chronic Kidney Disease or not based on 24 features. 
https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease

**Load Required Packages**
```{r}
library(RWeka)
library(caret)
# libraries for partition trees
library(rpart)
library(rpart.plot)
library(rattle)

library(ROCR)
```

**load data** 
```{r}
## load data
setwd("/Users/shruti/Desktop/WorkMiscellaneous/MachineLearning/UCI_ML/Chronic_Kidney_Disease")
Chronic_Kidney_Disease <- read.arff("chronic_kidney_disease.arff")

# data munging
dim(Chronic_Kidney_Disease)
head(Chronic_Kidney_Disease)
colnames(Chronic_Kidney_Disease)
summary(Chronic_Kidney_Disease)

# % data missing in each column
apply(Chronic_Kidney_Disease,2,function(i) {(sum(is.na(i))/nrow(Chronic_Kidney_Disease))*100})
# samples with no missing data
sum(complete.cases(Chronic_Kidney_Disease))
# samples with no missing data after removing columns which have more than 25% of data missing
sum(complete.cases(Chronic_Kidney_Disease[,-c(6,17,18)]))

### remove rows with any missing value
Chronic_Kidney_Disease2 <- Chronic_Kidney_Disease[complete.cases(Chronic_Kidney_Disease),]
dim(Chronic_Kidney_Disease2)
apply(Chronic_Kidney_Disease2[,c(3:9,19:25)],2,table)
```

**convert factor variables into dummy variables**
```{r}
# create dummy variables for training data
sapply(Chronic_Kidney_Disease2[1,],class)
# since many of the features are categorical, convert them into dummy varaibles. 
dummy_variables <- dummyVars(~.,data=Chronic_Kidney_Disease2,fullRank=T)
dataset_dummy_variables <- data.frame(predict(dummy_variables, newdata=Chronic_Kidney_Disease2))
table(Chronic_Kidney_Disease2$class)
table(dataset_dummy_variables$class.notckd) # 0 implies ckd; 1 implies notckd
```

**split data**
```{r}
set.seed(1)
training_index <- createDataPartition(dataset_dummy_variables$class.notckd,p=0.6,list=F)
# column id of the outcome
outcome_column_id <- grep("class.notckd",colnames(dataset_dummy_variables))

training_set <- droplevels(dataset_dummy_variables[training_index,-outcome_column_id])
test_set <- droplevels(dataset_dummy_variables[-training_index,-outcome_column_id])
dim(training_set); dim(test_set)

# if you are trying to do classification using regression models, it is imp to use outcome as factor not numeric
outcome_training_set <- factor(dataset_dummy_variables[training_index,outcome_column_id],ordered=TRUE)
outcome_test_set <- factor(dataset_dummy_variables[-training_index,outcome_column_id])
table(outcome_training_set) ; table(outcome_test_set)
```

**exploratory data analysis**
```{r}
# remove zero covaritates (features with no variability)
nearZeroVar(training_set,saveMetrics = T)
near_zero_covariates <- colnames(training_set)[nearZeroVar(training_set)]
#near_zero_covariates <- nearZeroVar(training_set)
near_zero_covariates

if(length(near_zero_covariates)>0)
{
  # find column indices of the near_zero_covariates
  nzc_indices_training <- sapply(near_zero_covariates,function(i) {grep(i,colnames(training_set))})
  training_set_nzc <- training_set[,-nzc_indices_training]
  
  nzc_indices_test <- sapply(near_zero_covariates,function(i) {grep(i,colnames(test_set))})  
  test_set_nzc <- test_set[,-nzc_indices_test]
} else {
  training_set_nzc <- training_set
  test_set_nzc <- test_set
}
dim(training_set_nzc); dim(test_set_nzc)

# correlated features
feature_correlation <- cor(training_set_nzc)
# search through a correlation matrix and returns a vector of integers corresponding to columns to remove to reduce pair-wise correlations.
findCorrelation(feature_correlation,0.75,verbose=T,names=T)
high_correlation <- findCorrelation(feature_correlation,0.75,verbose=T,names=T)
high_correlation

if(length(high_correlation)>0)
{
  correlated_indices_training <- sapply( high_correlation,function(i) {grep(i,colnames(training_set_nzc))} )
  final_training_set <- training_set_nzc[,-correlated_indices_training]
  
  correlated_indices_test <- sapply( high_correlation,function(i) {grep(i,colnames(test_set_nzc))} )
  final_test_set <- test_set_nzc[,-correlated_indices_test]
}else{
  final_training_set <- training_set_nzc
  final_test_set <- test_set_nzc
}
dim(final_training_set); dim(final_test_set)

# plots
# PCA
pc <- prcomp(final_training_set,center=T,scale=T)
plot(pc,type="l")
pc$rotation[order(-abs(pc$rotation[,"PC1"])),]

par(xpd=TRUE)
for(i in seq_along(colnames(final_training_set[,1:21])))
{
  plot(outcome_training_set,final_training_set[,i],main=colnames(final_training_set)[i])
  legend(1.2,-6,c("0:ckd","1:notckd"),cex=0.8)
}
```

**model building**
```{r: }
# k-fold cross validation
train_control <- trainControl(method="cv", number=5, savePredictions = T)

# model building
# svm - linear
set.seed(1)
svm_lm_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "svmLinear",preProcess = c("center", "scale","pca"))

# svm - rbf kernel
set.seed(1)
svm_rbf_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "svmRadial", tuneLegth=5, preProcess = c("center", "scale","pca"))

# logistic regression
set.seed(1)
glm_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "glm",family=binomial,maxit=100, preProcess = c("center", "scale","pca"))
# possible warnings with glm:
#1. "glm.fit: algorithm did not converge"; use maxit much greater than default value of 25"
#2. "glm.fit: fitted probabilities numerically 0 or 1 occurred" indicates that you should use penalized regression. Your fit is exactly collinear.
#3. "prediction from a rank-deficient fit may be misleading in:" indicates If the fit is rank-deficient, some of the columns of the design matrix will have been dropped. Prediction from such a fit only makes sense if 'newdata' is contained in the same subspace as the original data.

# Ridge Regression creates a linear regression model that is penalized with the L2-norm which is the sum of the squared coefficients.
set.seed(1)
ridge_regression_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "glmnet",family = "binomial",tuneGrid=expand.grid(alpha=0,lambda=0.001),preProcess = c("center", "scale","pca"))

# LASSO (Least Absolute Shrinkage and Selection Operator) creates a regression model that is penalized with the L1-norm which is the sum of the absolute coefficients. 
set.seed(1)
lasso_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "glmnet",family = "binomial",tuneGrid=expand.grid(alpha=1,lambda=0.001),preProcess = c("center", "scale","pca"))

# Elastic Net creates a regression model that is penalized with both the L1-norm and L2-norm. 
set.seed(1)
elastic_net_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "glmnet",family = "binomial",tuneGrid=expand.grid(alpha=0.5,lambda=0.001),preProcess = c("center", "scale","pca"))

# classification Trees
set.seed(1)
rpart_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "rpart",preProcess = c("center", "scale","pca"))
plot(rpart_model)

# random forest
set.seed(1)
rf_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "rf",prox=T,preProcess = c("center", "scale"))
plot(rf_model)

# boosting with tres
set.seed(1)
gbm_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "gbm", verbose=F,preProcess = c("center", "scale","pca"))
plot(gbm_model)

# linear discriminant analysis
set.seed(1)
lda_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "lda",preProcess = c("center", "scale","pca"))

# naive bayes
# parameters of the algorithm: fl: Factor for Laplace correction, default factor is 0, i.e. no correction;
# usekernel: if TRUE a kernel density estimate (density) is used for denstity estimation. If FALSE a normal density is estimated.
set.seed(1)
nb_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method="nb", tuneGrid=expand.grid(.fL=c(0), .usekernel=c(TRUE)) ,preProcess = c("center", "scale","pca"))
warnings()
# "The function NaiveBayes prints a warning if all these probabilities are numerical 0, i.e. that the observation has a numerical probability of 0 for all classes. "

# collect resamples
training_models <- list(SVM_LM=svm_lm_model,SVM_RBF=svm_rbf_model,RPART=rpart_model,GBM=gbm_model,RF=rf_model,LDA=lda_model,RIDGE=ridge_regression_model,LASSO=lasso_model,ELASTIC=elastic_net_model)
train_results <- resamples(training_models)
# summarize the distributions
summary(train_results)
# boxplots of results
bwplot(train_results)
# the above results suggest that svm-rf and random forest model performs best on the training data.
```

**EVALUATE MODEL ACCURACY ON TEST SET**
```{r}
#Ideally, you select model that performs best on training data and evaluate on test set. I am doing for all models just for illustration 
test_pred_svm_lm <- predict(svm_lm_model, newdata=final_test_set)
confusionMatrix(data=test_pred_svm_lm, reference=outcome_test_set)

test_pred_svm_rbf <- predict(svm_rbf_model, newdata=final_test_set)
confusionMatrix(data=test_pred_svm_rbf, reference=outcome_test_set)

test_pred_rpart <- predict(rpart_model, newdata=final_test_set)
confusionMatrix(data=test_pred_rpart, outcome_test_set)

test_pred_gbm <- predict(gbm_model, newdata=final_test_set)
confusionMatrix(data=test_pred_gbm, outcome_test_set)

test_pred_rf <- predict(rf_model, newdata=final_test_set)
confusionMatrix(data=test_pred_rf, outcome_test_set)

test_pred_lda <- predict(lda_model, newdata=final_test_set)
confusionMatrix(data=test_pred_lda, outcome_test_set)

test_pred_ridge <- predict(ridge_regression_model, newdata=final_test_set)
confusionMatrix(data=test_pred_ridge, outcome_test_set)

test_pred_lasso <- predict(lasso_model, newdata=final_test_set)
confusionMatrix(data=test_pred_lasso, outcome_test_set)

test_pred_elastic_net <- predict(elastic_net_model, newdata=final_test_set)
confusionMatrix(data=test_pred_elastic_net, outcome_test_set)

balanced_accuracy <- function(trained_model, test_features=final_test_set, test_outcomes=outcome_test_set){
  test_model <- predict(trained_model,test_features)
  test_score <- confusionMatrix(data=test_model, test_outcomes)
  return(test_score$byClass[["Balanced Accuracy"]])
}

lapply(training_models, balanced_accuracy)
```

**ROC CURVES**
```{r}
roc_curve <- function(test_predictions,colour=1,test_labels=outcome_test_set){
  pred <- prediction(as.numeric(test_predictions), as.numeric(test_labels) )
  perf <- performance(pred, measure = "tpr", x.measure = "fpr")
  plot(perf,col=colour)
}

roc_curve(test_pred_svm_rbf,colour=1)
par(new = TRUE)
roc_curve(test_pred_lda,colour=2)
par(new = TRUE)
roc_curve(test_pred_rf,colour=3)
par(new = TRUE)
roc_curve(test_pred_elastic_net,colour=4)
par(new = TRUE)
roc_curve(test_pred_gbm,colour=5)
par(new = FALSE)
legend("bottomright",c("svm radial kernel", "lda","random forest","elastic_net","graded boosting"), col = c(1:5),cex=0.8,lty=1)
title(main="ROC curves for test data")

# roc_curve(test_pred_svm_lm,colour=5)
# par(new = TRUE)
# roc_curve(test_pred_ridge,colour=6)
# par(new = TRUE)
# roc_curve(test_pred_lasso,colour=7)
# par(new = TRUE)
# roc_curve(test_pred_rpart,colour=8)
# par(new = FALSE)
# legend("bottomright",c("svm radial kernel", "lda","random forest","elastic_net","gbm","svm_lm","ridge","lasso","rpart"), col = c(2:9),cex=0.8,lty=1)
```

**Feature Importance**
```{r: Variable Importance}
varImp(svm_lm_model)
plot(varImp(svm_lm_model))

varImp(rf_model)
plot(varImp(rf_model))

set.seed(1)
rpart_model2 <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "rpart") # without scaling n pca
plot(varImp(rpart_model2))
# plot classification trees
fancyRpartPlot(rpart_model2$finalModel)
plot(Chronic_Kidney_Disease2$class,Chronic_Kidney_Disease2$pcv,main="pcv")
# summary(subset(Chronic_Kidney_Disease2,class=="ckd",pcv))
# summary(subset(Chronic_Kidney_Disease2,class=="notckd",pcv))

#lapply(training_models,varImp)
```

