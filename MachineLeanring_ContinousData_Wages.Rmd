---
title: "MachineLearning_ContinousData_Wages"
author: "Shruti"
date: "February 18, 2016"
---
Machine Learning - Continous Data example. Uses different machine learning algorithms to predict wage using multiple features.

**Load Required Packages**
```{r Chunk 1: load packages}
library(caret)
library(ISLR)
library(ggplot2)
library(Metrics) # RMSE
# libraries for partition trees
library(rpart)
library(rpart.plot)
library(rattle)
```

**split data**
```{r Chunk 2: split data}
data(Wage)
colnames(Wage)

set.seed(1)
training_indices <- createDataPartition(y=Wage$wage,p=0.8,list=F)
# remove log-wage column as feature 
training_set <- Wage[training_indices,-c(11)]
test_set <- Wage[-training_indices,-c(11)]
# dim(training_set); dim(test_set)
```

**exploratory data analysis**
```{r}
summary(Wage)
# remove near zero covariates (features with no variability)
nearZeroVar(training_set,saveMetrics = T)
near_zero_covariates <- nearZeroVar(training_set)
head(near_zero_covariates)
if(length(near_zero_covariates)>0)
{
  training_set2 <- training_set[,-near_zero_covariates]
  test_set2 <- test_set[,-near_zero_covariates]
} else {
  training_set2 <- training_set
  test_set2 <- test_set
}
dim(training_set2); dim(test_set2)

# exploratory plots
colnames(training_set2)
featurePlot(x=training_set2[1:8], y=training_set2$wage, plot="pairs")
qplot(x=training_set2$year, y=training_set2$wage ,ylab="wage")
qplot(x=training_set2$age, y=training_set2$wage ,ylab="wage")
qplot(x=training_set2$age, y=training_set2$wage, color=training_set2$education,pch=training_set2$jobclass ,ylab="wage")
# add regression smoother
qq <- qplot(x=training_set2$age, y=training_set2$wage, color=training_set2$education,pch=training_set2$jobclass ,ylab="wage")
qq + geom_smooth(method="lm",formula=y~x)

qplot(x=training_set2$maritl, y=training_set2$wage ,ylab="wage", color=training_set2$age)
qplot(x=training_set2$age, y=training_set2$wage, color=training_set2$maritl ,ylab="wage")

qplot(x=training_set2$race,y=training_set2$wage,color=training_set2$education ,pch=training_set2$jobclass,cex=1,ylab="wage")
qplot(x=training_set2$health,y=training_set2$wage,color=training_set2$health_ins ,pch=training_set2$jobclass,cex=1,ylab="wage")

# correlated features
# all features should be numeric to calculate correlation
sapply(training_set2[1,],class)
numeric_set <- which(sapply(training_set2[1,],class)=="integer")

feature_correlation <- cor(training_set2[,numeric_set])
# search through a correlation matrix and returns a vector of integers corresponding to columns to remove to reduce pair-wise correlations.
findCorrelation(feature_correlation,0.75,verbose=T,names=T)
high_correlation <- findCorrelation(feature_correlation,0.75,verbose=T,names=T)
high_correlation
# grep(c(high_correlation[1]),colnames(training_set2))
# grep(c(high_correlation[2]),colnames(training_set2))
if(length(high_correlation)>0)
{
  training_set3 <- training_set2[,-c(11,15)]
  test_set3 <- test_set2[,-c(11,15)]
}else{
  training_set3 <- training_set2
  test_set3 <- test_set2
}
dim(training_set3); dim(test_set3)

# PCA
pc <- prcomp(training_set3[,numeric_set],center=T,scale=T)
plot(pc,type="l")
pc$rotation[order(-abs(pc$rotation[,"PC1"])),]
```

**model building**
```{r}
# since correlation and PCA shows all features are important, all of them are used in the model
# k-fold cross validation
train_control <- trainControl(method="cv", number=10, savePredictions = T)
# fix the parameters of the algorithm
grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))

# linear regression
set.seed(1)
lm_model <- train(wage~., data=training_set3, trControl=train_control, method="lm" ,preProcess=c("scale","center","pca"))

lm_model
# summary(lm_model)
# lm_model$finalModel
varImp(lm_model)
plot(varImp(lm_model))

# re-train model using features that look important
set.seed(1)
lm_model2 <- train(wage ~ education + jobclass + health_ins + maritl, data=training_set3, trControl=train_control, method="lm")

# tree / recursive partioning
set.seed(1)
rpart_model <- train(wage~., data=training_set3, trControl=train_control, method="rpart" ,preProcess=c("scale","center","pca"))
# plot classification trees
fancyRpartPlot(rpart_model$finalModel)

# boosting with tres
set.seed(1)
gbm_model <- train(wage~., data=training_set3, trControl=train_control, method = "gbm", verbose=F ,preProcess=c("scale","center","pca"))

# random forest. this will take time for high "k" fold cv
set.seed(1)
rf_model <- train(wage~., data=training_set3, trControl=train_control, method="rf" ,preProcess=c("scale","center","pca"))

# lasso - regularized regression
set.seed(1)
lasso_model <- train(wage~., data=training_set3, trControl=train_control, method="lasso", metric="RMSE" ,preProcess=c("scale","center","pca"))

# collect resamples
train_results <- resamples(list(LM=lm_model,LM2=lm_model2,RPART=rpart_model,GBM=gbm_model,RF=rf_model))
# summarize the distributions
summary(train_results)
# boxplots of results
bwplot(train_results)
# the above results suggest that LM n GBM model performs best on the training data.
```

**EVALUATE MODEL ACCURACY ON TEST SET**
```{r}
#Ideally, you select model that performs best on training data and evaluate on test set. I am doing for all models just for illustration 
par(mfrow=c(2,2))
test_pred_lm <- predict(lm_model, newdata=test_set3)
rmse(test_set3$wage,test_pred_lm)
plot(test_pred_lm,test_set3$wage)

test_pred_lm2 <- predict(lm_model2, newdata=test_set3)
rmse(test_set3$wage,test_pred_lm2)
plot(test_pred_lm2,test_set3$wage)

test_pred_rpart <- predict(rpart_model, newdata=test_set3)
rmse(test_set3$wage,test_pred_rpart)
plot(test_pred_rpart,test_set3$wage)

test_pred_gbm <- predict(gbm_model, newdata=test_set3)
rmse(test_set3$wage,test_pred_gbm)
plot(test_pred_gbm,test_set3$wage)

test_pred_rf <- predict(rf_model, newdata=test_set3)
rmse(test_set3$wage,test_pred_rf)
plot(test_pred_rf,test_set3$wage)
```


