---
title: "ChronicKidneyDiseasePrediction_ML"
author: "Shruti"
date: "February 25, 2016"
output: html_document
---

Machine Learning Classification Problem example. Uses different machine learning algorithms to classify patients with Chronic Kidney Disease or not based on 24 features. 
https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease

**Load Required Packages**
```{r}
library(RWeka)
library(caret)
# libraries for partition trees
library(rpart)
library(rpart.plot)
library(rattle)
```

**load data** 
```{r}
## load data
setwd("/Users/shruti/Desktop/WorkMiscellaneous/MachineLearning/UCI_ML/Chronic_Kidney_Disease")
Chronic_Kidney_Disease <- read.arff("chronic_kidney_disease.arff")

# data munging
dim(Chronic_Kidney_Disease)
head(Chronic_Kidney_Disease)
colnames(Chronic_Kidney_Disease)
summary(Chronic_Kidney_Disease)

# % data missing in each column
apply(Chronic_Kidney_Disease,2,function(i) {(sum(is.na(i))/nrow(Chronic_Kidney_Disease))*100})
# samples with no missing data
sum(complete.cases(Chronic_Kidney_Disease))
# samples with no missing data after removing columns which have more than 25% of data missing
sum(complete.cases(Chronic_Kidney_Disease[,-c(6,17,18)]))

### remove rows with any missing value
Chronic_Kidney_Disease2 <- Chronic_Kidney_Disease[complete.cases(Chronic_Kidney_Disease),]
dim(Chronic_Kidney_Disease2)
apply(Chronic_Kidney_Disease2[,c(3:9,19:25)],2,table)
```

**convert factor variables into dummy variables**
```{r}
# create dummy variables for training data
sapply(Chronic_Kidney_Disease2[1,],class)
# since many of the features are categorical, convert them into dummy varaibles. 
dummy_variables <- dummyVars(~.,data=Chronic_Kidney_Disease2,fullRank=T)
dataset_dummy_variables <- data.frame(predict(dummy_variables, newdata=Chronic_Kidney_Disease2))
```

**split data**
```{r}
set.seed(1)
training_index <- createDataPartition(dataset_dummy_variables$class.notckd,p=0.8,list=F)
# column id of the outcome
outcome_column_id <- grep("class.notckd",colnames(dataset_dummy_variables))

training_set <- droplevels(dataset_dummy_variables[training_index,-outcome_column_id])
test_set <- droplevels(dataset_dummy_variables[-training_index,-outcome_column_id])
dim(training_set); dim(test_set)

# if you are trying to do classification using regression models, it is imp to use outcome as factor not numeric
outcome_training_set <- as.factor(dataset_dummy_variables[training_index,outcome_column_id])
outcome_test_set <- as.factor(dataset_dummy_variables[-training_index,outcome_column_id])
```

**exploratory data analysis**
```{r}
# remove zero covaritates (features with no variability)
nearZeroVar(training_set,saveMetrics = T)
near_zero_covariates <- colnames(training_set)[nearZeroVar(training_set)]
#near_zero_covariates <- nearZeroVar(training_set)
near_zero_covariates

if(length(near_zero_covariates)>0)
{
  # find column indices of the near_zero_covariates
  nzc_indices_training <- sapply(near_zero_covariates,function(i) {grep(i,colnames(training_set))})
  training_set_nzc <- training_set[,-nzc_indices_training]
  
  nzc_indices_test <- sapply(near_zero_covariates,function(i) {grep(i,colnames(test_set))})  
  test_set_nzc <- test_set[,-nzc_indices_test]
} else {
  training_set_nzc <- training_set
  test_set_nzc <- test_set
}
dim(training_set_nzc); dim(test_set_nzc)

# correlated features
feature_correlation <- cor(training_set_nzc)
# search through a correlation matrix and returns a vector of integers corresponding to columns to remove to reduce pair-wise correlations.
findCorrelation(feature_correlation,0.75,verbose=T,names=T)
high_correlation <- findCorrelation(feature_correlation,0.75,verbose=T,names=T)
high_correlation

if(length(high_correlation)>0)
{
  correlated_indices_training <- sapply( high_correlation,function(i) {grep(i,colnames(training_set_nzc))} )
  final_training_set <- training_set_nzc[,-correlated_indices_training]
  
  correlated_indices_test <- sapply( high_correlation,function(i) {grep(i,colnames(test_set_nzc))} )
  final_test_set <- test_set_nzc[,-correlated_indices_test]
}else{
  final_training_set <- training_set_nzc
  final_test_set <- test_set_nzc
}
dim(final_training_set); dim(final_test_set)

# plots
# PCA
pc <- prcomp(final_training_set,center=T,scale=T)
plot(pc,type="l")
pc$rotation[order(-abs(pc$rotation[,"PC1"])),]

par(xpd=TRUE)
for(i in seq_along(colnames(final_training_set[,1:21])))
{
  plot(outcome_training_set,final_training_set[,i],main=colnames(final_training_set)[i])
  legend(1.2,-6,c("0:ckd","1:notckd"),cex=0.8)
}
```

**model building**
```{r: }
# k-fold cross validation
train_control <- trainControl(method="cv", number=5, savePredictions = T)

# model building
# svm - linear
set.seed(1)
svm_lm_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "svmLinear",preProcess = c("center", "scale","pca"))

svm_lm_model
varImp(svm_lm_model)
plot(varImp(svm_lm_model))

# svm - rbf kernel
set.seed(1)
svm_rbf_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "svmRadial",preProcess = c("center", "scale","pca"))

# logistic regression
set.seed(1)
glm_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "glm",family=binomial,maxit=100)
# possible warnings with glm:
#1. "glm.fit: algorithm did not converge"; use maxit much greater than default value of 25"
#2. "glm.fit: fitted probabilities numerically 0 or 1 occurred" indicates that you should use penalized regression. Your fit is exactly collinear.
#3. "prediction from a rank-deficient fit may be misleading in:" indicates If the fit is rank-deficient, some of the columns of the design matrix will have been dropped. Prediction from such a fit only makes sense if 'newdata' is contained in the same subspace as the original data.

# Ridge Regression creates a linear regression model that is penalized with the L2-norm which is the sum of the squared coefficients.
set.seed(1)
ridge_regression_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "glmnet",family = "binomial",tuneGrid=expand.grid(alpha=0,lambda=0.001))

# LASSO (Least Absolute Shrinkage and Selection Operator) creates a regression model that is penalized with the L1-norm which is the sum of the absolute coefficients. 
set.seed(1)
lasso_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "glmnet",family = "binomial",tuneGrid=expand.grid(alpha=1,lambda=0.001))

# Elastic Net creates a regression model that is penalized with both the L1-norm and L2-norm. 
set.seed(1)
elastic_net_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "glmnet",family = "binomial",tuneGrid=expand.grid(alpha=0.5,lambda=0.001))

# classification Trees
set.seed(1)
rpart_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "rpart")
plot(varImp(rpart_model))
# plot classification trees
fancyRpartPlot(rpart_model$finalModel)
# summary(subset(final_training_set,class=="ckd",hemo))
# summary(subset(final_training_set,class=="notckd",hemo))
# plot(final_training_set$class,final_training_set$hemo)
plot(rpart_model)

# random forest
set.seed(1)
rf_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "rf",prox=T)
plot(rf_model)

# boosting with tres
set.seed(1)
gbm_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "gbm", verbose=F)
plot(gbm_model)

# linear discriminant analysis
set.seed(1)
lda_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method = "lda",preProcess = c("center", "scale","pca"))

# naive bayes
# parameters of the algorithm: fl: Factor for Laplace correction, default factor is 0, i.e. no correction;
# usekernel: if TRUE a kernel density estimate (density) is used for denstity estimation. If FALSE a normal density is estimated.
set.seed(1)
nb_model <- train(y=outcome_training_set, x=final_training_set, trControl=train_control, method="nb", tuneGrid=expand.grid(.fL=c(0), .usekernel=c(TRUE)))
warnings()
# "The function NaiveBayes prints a warning if all these probabilities are numerical 0, i.e. that the observation has a numerical probability of 0 for all classes. "

# collect resamples
training_models <- list(SVM_LM=svm_lm_model,SVM_RBF=svm_rbf_model,RPART=rpart_model,GBM=gbm_model,RF=rf_model,LDA=lda_model,RIDGE=ridge_regression_model,LASSO=lasso_model,ELASTIC=elastic_net_model)
train_results <- resamples(training_models)
# summarize the distributions
summary(train_results)
# boxplots of results
bwplot(train_results)
# the above results suggest that random forest model performs best on the training data.

lapply(training_models,varImp)
```

**EVALUATE MODEL ACCURACY ON TEST SET**
```{r}
#Ideally, you select model that performs best on training data and evaluate on test set. I am doing for all models just for illustration 
test_pred_svm_lm <- predict(svm_lm_model, newdata=final_test_set)
confusionMatrix(data=test_pred_svm_lm, outcome_test_set)

test_pred_svm_rbf <- predict(svm_rbf_model, newdata=final_test_set)
confusionMatrix(data=test_pred_svm_rbf, outcome_test_set)

test_pred_rpart <- predict(rpart_model, newdata=final_test_set)
confusionMatrix(data=test_pred_rpart, outcome_test_set)

test_pred_gbm <- predict(gbm_model, newdata=final_test_set)
confusionMatrix(data=test_pred_gbm, outcome_test_set)

test_pred_rf <- predict(rf_model, newdata=final_test_set)
confusionMatrix(data=test_pred_rf, outcome_test_set)

test_pred_lda <- predict(lda_model, newdata=final_test_set)
confusionMatrix(data=test_pred_lda, outcome_test_set)

balanced_accuracy <- function(trained_model, test_features=final_test_set, test_outcomes=outcome_test_set){
  test_model <- predict(trained_model,test_features)
  test_score <- confusionMatrix(data=test_model, test_outcomes)
  return(test_score$byClass[["Balanced Accuracy"]])
}

lapply(training_models, balanced_accuracy)
```
